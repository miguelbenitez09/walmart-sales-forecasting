{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89648601",
   "metadata": {},
   "source": [
    "## üì¶ 1. Importar Librer√≠as\n",
    "\n",
    "pandas, numpy, sklearn para manipulaci√≥n de datos y feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6391a948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:45.189856Z",
     "iopub.status.busy": "2025-12-30T03:39:45.189856Z",
     "iopub.status.idle": "2025-12-30T03:39:46.177402Z",
     "shell.execute_reply": "2025-12-30T03:39:46.177402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as cargadas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print('‚úÖ Librer√≠as cargadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab6ffb",
   "metadata": {},
   "source": [
    "## üìÇ 2. Cargar Datos\n",
    "\n",
    "Carga de train, test, features y stores desde data/01_raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9a9e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:46.179459Z",
     "iopub.status.busy": "2025-12-30T03:39:46.179459Z",
     "iopub.status.idle": "2025-12-30T03:39:46.410113Z",
     "shell.execute_reply": "2025-12-30T03:39:46.410113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (421570, 5), Test: (115064, 4), Features: (8190, 12), Stores: (45, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos cargados\n"
     ]
    }
   ],
   "source": [
    "RAW_PATH = '../data/01_raw/'\n",
    "PROCESSED_PATH = '../data/02_processed/'\n",
    "\n",
    "train = pd.read_csv(RAW_PATH + 'train.csv')\n",
    "test = pd.read_csv(RAW_PATH + 'test.csv')\n",
    "features = pd.read_csv(RAW_PATH + 'features.csv')\n",
    "stores = pd.read_csv(RAW_PATH + 'stores.csv')\n",
    "\n",
    "print(f'Train: {train.shape}, Test: {test.shape}, Features: {features.shape}, Stores: {stores.shape}')\n",
    "\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "test['Date'] = pd.to_datetime(test['Date'])\n",
    "features['Date'] = pd.to_datetime(features['Date'])\n",
    "print('‚úÖ Datos cargados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7b3c8",
   "metadata": {},
   "source": [
    "## üîó 3. Merge Datasets\n",
    "\n",
    "Union de train/test con features (por Store, Date, IsHoliday) y stores (por Store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a1625c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:46.412128Z",
     "iopub.status.busy": "2025-12-30T03:39:46.412128Z",
     "iopub.status.idle": "2025-12-30T03:39:46.568849Z",
     "shell.execute_reply": "2025-12-30T03:39:46.568849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (421570, 16), Test: (115064, 15)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = train.merge(features, on=['Store','Date','IsHoliday'], how='left')\n",
    "df_train = df_train.merge(stores, on='Store', how='left')\n",
    "df_test = test.merge(features, on=['Store','Date','IsHoliday'], how='left')\n",
    "df_test = df_test.merge(stores, on='Store', how='left')\n",
    "print(f'Train: {df_train.shape}, Test: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0894472",
   "metadata": {},
   "source": [
    "## üßπ 4. Rellenar MarkDowns con 0\n",
    "\n",
    "Los valores faltantes en promociones se asumen como 0 (sin descuento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3128a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:46.570854Z",
     "iopub.status.busy": "2025-12-30T03:39:46.570854Z",
     "iopub.status.idle": "2025-12-30T03:39:46.585019Z",
     "shell.execute_reply": "2025-12-30T03:39:46.585019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MarkDowns rellenados con 0\n"
     ]
    }
   ],
   "source": [
    "markdown_cols = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n",
    "for col in markdown_cols:\n",
    "    df_train[col].fillna(0, inplace=True)\n",
    "    df_test[col].fillna(0, inplace=True)\n",
    "print('‚úÖ MarkDowns rellenados con 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b9b65",
   "metadata": {},
   "source": [
    "## ‚ö° 5. Features Temporales\n",
    "\n",
    "16 features: Year, Month, Week, Quarter, DayOfWeek, sin/cos encoding, Trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3446941d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:46.586537Z",
     "iopub.status.busy": "2025-12-30T03:39:46.586537Z",
     "iopub.status.idle": "2025-12-30T03:39:46.831624Z",
     "shell.execute_reply": "2025-12-30T03:39:46.831624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features temporales creadas\n"
     ]
    }
   ],
   "source": [
    "def create_temporal_features(df):\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Week'] = df['Date'].dt.isocalendar().week\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "    df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
    "    df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
    "    df['IsQuarterStart'] = df['Date'].dt.is_quarter_start.astype(int)\n",
    "    df['IsQuarterEnd'] = df['Date'].dt.is_quarter_end.astype(int)\n",
    "    df['WeekOfMonth'] = (df['Date'].dt.day - 1) // 7 + 1\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
    "    df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
    "    df['Trend'] = (df['Date'] - df['Date'].min()).dt.days // 7\n",
    "    return df\n",
    "\n",
    "df_train = create_temporal_features(df_train)\n",
    "df_test = create_temporal_features(df_test)\n",
    "print('‚úÖ Features temporales creadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198e415",
   "metadata": {},
   "source": [
    "## üéÑ 6. Features de Festivos\n",
    "\n",
    "8 features: banderas para cada festivo + d√≠as hasta/desde festivo + pre/post-festivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68b9053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:46.835145Z",
     "iopub.status.busy": "2025-12-30T03:39:46.835145Z",
     "iopub.status.idle": "2025-12-30T03:39:59.361575Z",
     "shell.execute_reply": "2025-12-30T03:39:59.361575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features de festivos creadas\n"
     ]
    }
   ],
   "source": [
    "def create_holiday_features(df):\n",
    "    holidays = {\n",
    "        'SuperBowl': ['2010-02-12','2011-02-11','2012-02-10'],\n",
    "        'LaborDay': ['2010-09-10','2011-09-09','2012-09-07'],\n",
    "        'Thanksgiving': ['2010-11-26','2011-11-25','2012-11-23'],\n",
    "        'Christmas': ['2010-12-31','2011-12-30','2012-12-28']\n",
    "    }\n",
    "    for name, dates in holidays.items():\n",
    "        dates = [pd.to_datetime(d) for d in dates]\n",
    "        df[f'Is{name}'] = df['Date'].isin(dates).astype(int)\n",
    "    \n",
    "    all_holidays = []\n",
    "    for dates in holidays.values():\n",
    "        all_holidays.extend([pd.to_datetime(d) for d in dates])\n",
    "    all_holidays = sorted(set(all_holidays))\n",
    "    \n",
    "    df['DaysToNextHoliday'] = df['Date'].apply(lambda x: min([(h-x).days for h in all_holidays if h>=x], default=365))\n",
    "    df['DaysFromLastHoliday'] = df['Date'].apply(lambda x: min([(x-h).days for h in all_holidays if h<=x], default=365))\n",
    "    df['IsPreHoliday'] = (df['DaysToNextHoliday'] <= 7).astype(int)\n",
    "    df['IsPostHoliday'] = (df['DaysFromLastHoliday'] <= 7).astype(int)\n",
    "    return df\n",
    "\n",
    "df_train = create_holiday_features(df_train)\n",
    "df_test = create_holiday_features(df_test)\n",
    "print('‚úÖ Features de festivos creadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48894834",
   "metadata": {},
   "source": [
    "## üìä 7. Features de Lag (Solo Train)\n",
    "\n",
    "4 features: ventas de 1, 2, 3, 4 semanas previas (solo para entrenamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d1eadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:59.363596Z",
     "iopub.status.busy": "2025-12-30T03:39:59.363596Z",
     "iopub.status.idle": "2025-12-30T03:39:59.594981Z",
     "shell.execute_reply": "2025-12-30T03:39:59.594675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lag features creadas\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.sort_values(['Store','Dept','Date'])\n",
    "for lag in [1,2,3,4]:\n",
    "    df_train[f'Weekly_Sales_Lag{lag}'] = df_train.groupby(['Store','Dept'])['Weekly_Sales'].shift(lag)\n",
    "print('‚úÖ Lag features creadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595ee02",
   "metadata": {},
   "source": [
    "## üìà 8. Rolling Features (Solo Train)\n",
    "\n",
    "12 features: promedio, std, min, max m√≥viles para ventanas de 4, 8, 12 semanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00fd4dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:39:59.597494Z",
     "iopub.status.busy": "2025-12-30T03:39:59.597494Z",
     "iopub.status.idle": "2025-12-30T03:40:06.767153Z",
     "shell.execute_reply": "2025-12-30T03:40:06.767153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rolling features creadas\n"
     ]
    }
   ],
   "source": [
    "for window in [4,8,12]:\n",
    "    df_train[f'Weekly_Sales_RollingMean{window}'] = df_train.groupby(['Store','Dept'])['Weekly_Sales'].transform(lambda x: x.shift(1).rolling(window, min_periods=1).mean())\n",
    "    df_train[f'Weekly_Sales_RollingStd{window}'] = df_train.groupby(['Store','Dept'])['Weekly_Sales'].transform(lambda x: x.shift(1).rolling(window, min_periods=1).std())\n",
    "    df_train[f'Weekly_Sales_RollingMin{window}'] = df_train.groupby(['Store','Dept'])['Weekly_Sales'].transform(lambda x: x.shift(1).rolling(window, min_periods=1).min())\n",
    "    df_train[f'Weekly_Sales_RollingMax{window}'] = df_train.groupby(['Store','Dept'])['Weekly_Sales'].transform(lambda x: x.shift(1).rolling(window, min_periods=1).max())\n",
    "print('‚úÖ Rolling features creadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821529b",
   "metadata": {},
   "source": [
    "## üè¨ 9. Features Agregadas Store-Dept\n",
    "\n",
    "5 features: estad√≠sticas hist√≥ricas (mean, std, min, max, median) por tienda y departamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896b9659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:40:06.769144Z",
     "iopub.status.busy": "2025-12-30T03:40:06.769144Z",
     "iopub.status.idle": "2025-12-30T03:40:07.172640Z",
     "shell.execute_reply": "2025-12-30T03:40:07.171633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features agregadas creadas\n"
     ]
    }
   ],
   "source": [
    "agg_stats = df_train.groupby(['Store','Dept'])['Weekly_Sales'].agg([('StoreDept_Mean','mean'),('StoreDept_Std','std'),('StoreDept_Min','min'),('StoreDept_Max','max'),('StoreDept_Median','median')]).reset_index()\n",
    "df_train = df_train.merge(agg_stats, on=['Store','Dept'], how='left')\n",
    "df_test = df_test.merge(agg_stats, on=['Store','Dept'], how='left')\n",
    "print('‚úÖ Features agregadas creadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a69fcd",
   "metadata": {},
   "source": [
    "## üîÄ 10. Features de Interacci√≥n\n",
    "\n",
    "7 features: combinaciones como Type_Holiday, Total_MarkDown, Temp_Month, Econ_Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bb74cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:40:07.176164Z",
     "iopub.status.busy": "2025-12-30T03:40:07.175162Z",
     "iopub.status.idle": "2025-12-30T03:40:07.790145Z",
     "shell.execute_reply": "2025-12-30T03:40:07.789436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features de interacci√≥n creadas\n"
     ]
    }
   ],
   "source": [
    "def create_interaction_features(df):\n",
    "    df['Type_Holiday'] = df['Type'].astype(str) + '_' + df['IsHoliday'].astype(str)\n",
    "    df['Store_Dept'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
    "    df['Temp_Month'] = df['Temperature'] * df['Month']\n",
    "    df['Size_Holiday'] = df['Size'] * df['IsHoliday']\n",
    "    df['Total_MarkDown'] = df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].sum(axis=1)\n",
    "    df['Count_MarkDown'] = (df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']] > 0).sum(axis=1)\n",
    "    df['Econ_Index'] = df['Unemployment'] * df['CPI']\n",
    "    return df\n",
    "\n",
    "df_train = create_interaction_features(df_train)\n",
    "df_test = create_interaction_features(df_test)\n",
    "print('‚úÖ Features de interacci√≥n creadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98aa6e7",
   "metadata": {},
   "source": [
    "## üî§ 11. Encoding Categ√≥ricas\n",
    "\n",
    "LabelEncoding para Type, Type_Holiday, Store_Dept (convertir texto a n√∫meros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0acaf90f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:40:07.793726Z",
     "iopub.status.busy": "2025-12-30T03:40:07.793726Z",
     "iopub.status.idle": "2025-12-30T03:44:14.772948Z",
     "shell.execute_reply": "2025-12-30T03:44:14.772442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoding completado\n"
     ]
    }
   ],
   "source": [
    "for col in ['Type','Type_Holiday','Store_Dept']:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col+'_Encoded'] = le.fit_transform(df_train[col].astype(str))\n",
    "    df_test[col+'_Encoded'] = df_test[col].astype(str).apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "print('‚úÖ Encoding completado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08566f",
   "metadata": {},
   "source": [
    "## üßπ 12. Limpieza de NaNs\n",
    "\n",
    "Relleno de valores faltantes en lag/rolling features con promedio por Store-Dept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34961714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:44:14.776523Z",
     "iopub.status.busy": "2025-12-30T03:44:14.776523Z",
     "iopub.status.idle": "2025-12-30T03:44:23.342082Z",
     "shell.execute_reply": "2025-12-30T03:44:23.341071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs restantes train: 0\n",
      "NaNs restantes test: 114630\n"
     ]
    }
   ],
   "source": [
    "lag_rolling_cols = [col for col in df_train.columns if 'Lag' in col or 'Rolling' in col]\n",
    "for col in lag_rolling_cols:\n",
    "    df_train[col] = df_train.groupby(['Store','Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    df_train[col].fillna(0, inplace=True)\n",
    "\n",
    "if 'StoreDept_Std' in df_train.columns:\n",
    "    df_train['StoreDept_Std'].fillna(0, inplace=True)\n",
    "    df_test['StoreDept_Std'].fillna(0, inplace=True)\n",
    "\n",
    "print(f'NaNs restantes train: {df_train.isnull().sum().sum()}')\n",
    "print(f'NaNs restantes test: {df_test.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b15ba",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 13. Train/Validation Split\n",
    "\n",
    "Divisi√≥n temporal: 85% para entrenamiento, 15% para validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f33e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:44:23.346204Z",
     "iopub.status.busy": "2025-12-30T03:44:23.346204Z",
     "iopub.status.idle": "2025-12-30T03:44:24.523247Z",
     "shell.execute_reply": "2025-12-30T03:44:24.523247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split date: 2012-06-01 00:00:00\n",
      "Train: 356,489 | Validation: 65,081\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.sort_values('Date')\n",
    "train_dates = sorted(df_train['Date'].unique())\n",
    "split_idx = int(len(train_dates) * 0.85)\n",
    "split_date = train_dates[split_idx]\n",
    "\n",
    "X_train_full = df_train[df_train['Date'] < split_date].copy()\n",
    "X_val = df_train[df_train['Date'] >= split_date].copy()\n",
    "\n",
    "print(f'Split date: {split_date}')\n",
    "print(f'Train: {X_train_full.shape[0]:,} | Validation: {X_val.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59cfc14",
   "metadata": {},
   "source": [
    "## üìä 14. Preparar X, y para Modelado\n",
    "\n",
    "Separaci√≥n de features (X) y target (y), eliminando columnas no necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "071b2703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:44:24.526669Z",
     "iopub.status.busy": "2025-12-30T03:44:24.526669Z",
     "iopub.status.idle": "2025-12-30T03:44:24.630023Z",
     "shell.execute_reply": "2025-12-30T03:44:24.628479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (356489, 66) | X_val: (65081, 66)\n",
      "y_train: (356489,) | y_val: (65081,)\n",
      "Total features: 66\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = ['Date','Weekly_Sales','Type','Type_Holiday','Store_Dept']\n",
    "y_train = X_train_full['Weekly_Sales']\n",
    "y_val = X_val['Weekly_Sales']\n",
    "X_train = X_train_full.drop(columns=features_to_drop, errors='ignore')\n",
    "X_val_features = X_val.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "print(f'X_train: {X_train.shape} | X_val: {X_val_features.shape}')\n",
    "print(f'y_train: {y_train.shape} | y_val: {y_val.shape}')\n",
    "print(f'Total features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666c84f",
   "metadata": {},
   "source": [
    "## üì¶ 15. Exportar Datos Procesados\n",
    "\n",
    "Exportaci√≥n a data/02_processed/: train/val/test procesados y X/y separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38904449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T03:44:24.636596Z",
     "iopub.status.busy": "2025-12-30T03:44:24.635087Z",
     "iopub.status.idle": "2025-12-30T03:45:01.984867Z",
     "shell.execute_reply": "2025-12-30T03:45:01.984867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos exportados a: ../data/02_processed/\n",
      "  test_processed.csv (36.59 MB)\n",
      "  train_processed.csv (173.97 MB)\n",
      "  val_processed.csv (32.79 MB)\n",
      "  X_train.csv (162.21 MB)\n",
      "  X_val.csv (30.64 MB)\n",
      "  y_train.csv (3.07 MB)\n",
      "  y_val.csv (0.56 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "X_train_full.to_csv(PROCESSED_PATH + 'train_processed.csv', index=False)\n",
    "X_val.to_csv(PROCESSED_PATH + 'val_processed.csv', index=False)\n",
    "df_test.to_csv(PROCESSED_PATH + 'test_processed.csv', index=False)\n",
    "X_train.to_csv(PROCESSED_PATH + 'X_train.csv', index=False)\n",
    "X_val_features.to_csv(PROCESSED_PATH + 'X_val.csv', index=False)\n",
    "y_train.to_csv(PROCESSED_PATH + 'y_train.csv', index=False, header=['Weekly_Sales'])\n",
    "y_val.to_csv(PROCESSED_PATH + 'y_val.csv', index=False, header=['Weekly_Sales'])\n",
    "\n",
    "print(f'‚úÖ Datos exportados a: {PROCESSED_PATH}')\n",
    "for f in os.listdir(PROCESSED_PATH):\n",
    "    if f.endswith('.csv'):\n",
    "        size_mb = os.path.getsize(PROCESSED_PATH+f)/(1024**2)\n",
    "        print(f'  {f} ({size_mb:.2f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e604f",
   "metadata": {},
   "source": [
    "## ‚úÖ Resumen Final\n",
    "\n",
    "**FEATURES CREADAS:**\n",
    "- Temporales: 16 (Year, Month, Week, Quarter, etc.)\n",
    "- Festivos: 8 (IsSuperBowl, IsLaborDay, etc.)\n",
    "- Lag: 4 (1-4 semanas atr√°s)\n",
    "- Rolling: 12 (mean, std, min, max para 4/8/12 semanas)\n",
    "- Agregadas: 5 (StoreDept statistics)\n",
    "- Interacci√≥n: 7 (Type_Holiday, Total_MarkDown, etc.)\n",
    "- Encoding: 3 (Type_Encoded, etc.)\n",
    "\n",
    "**TOTAL: ~{X_train.shape[1]} features**\n",
    "\n",
    "**PR√ìXIMO:** 03_modelado_dataset.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
